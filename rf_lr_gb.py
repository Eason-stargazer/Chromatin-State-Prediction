# -*- coding: utf-8 -*-
"""Chromatin State Prediction_RF/LR/GB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y0_jyIiy1v_9Mf6caCraM7zveWPfncv1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

df_seq = pd.read_csv("trainsequences.csv", names=["sequence"])
def compute_cg_features(seq):
    L = len(seq)
    return [
        seq.count("C") / L,
        seq.count("G") / L,
        seq.count("CG") / (L - 1),
        seq.count("GC") / (L - 1),
    ]
cg_features = np.array(
    [compute_cg_features(s) for s in df_seq["sequence"]]
)

"""Extract Kmers and Add Features"""

df = pd.read_csv("trainlabels.csv", names = ["labels"])
df1 = pd.read_csv("kmer_train_counts1.csv")
df2 = pd.read_csv("kmer_train_counts2.csv")
df3 = pd.read_csv("kmer_train_counts3.csv")
df4 = pd.read_csv("kmer_train_counts4.csv")

df = pd.concat([df4,df3,df2,df1,df], axis = 1)

cg_df = pd.DataFrame(
    cg_features,
    columns=["C_frac", "G_frac", "CG_frac", "GC_frac"]
)

df = pd.concat([df.iloc[:, :-1], cg_df, df.iloc[:, -1]], axis=1)
df.head(10)

"""Scale and Split the Dataset"""

def scale_dataset(dataframe, oversample=False):
    X = dataframe[dataframe.columns[:-1]].values
    y = dataframe[dataframe.columns[-1]].values

    SAMPLE_SIZE = 100000

    # Take 50k sample
    X, X_rest, y, y_rest = train_test_split(
        X,
        y,
        train_size=SAMPLE_SIZE,
        random_state=42,
        stratify=y
    )

    # Split the 50k sample (NOT X_rest)
    X_train, X_val, y_train, y_val = train_test_split(
        X,
        y,
        test_size=0.20,
        random_state=42,
        stratify=y  # stratify must match y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    if oversample:
        ros = RandomOverSampler(random_state=42)
        X_train_scaled, y_train = ros.fit_resample(
            X_train_scaled, y_train
        )

    return X_train_scaled, X_val_scaled, y_train, y_val

X_train_scaled, X_val_scaled, y_train, y_val = scale_dataset(df, oversample=False)

"""Logistic Regression Model & Validation"""

lg_model = LogisticRegression(
    solver='lbfgs',
    max_iter=3000, # Significantly increased iterations
    n_jobs=-1)
lg_model = lg_model.fit(X_train_scaled,y_train)

validation_accuracy = lg_model.score(X_val_scaled, y_val)
print(f"Logistic Regression Validation Accuracy: {validation_accuracy:.4f}")

"""Random Forest & Validation"""

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=20,
    min_samples_leaf=10,
    random_state=42,
    bootstrap=False,
    n_jobs=-1          # Use all CPU cores for speed
)
rf_model.fit(X_train_scaled, y_train)

validation_accuracy = rf_model.score(X_val_scaled, y_val)
print("Train:", rf_model.score(X_train_scaled, y_train))
print(f"\nRandom Forest Validation Accuracy: {validation_accuracy:.4f}")

"""Gradient & Validation"""

from sklearn.ensemble import HistGradientBoostingClassifier

gb_model = HistGradientBoostingClassifier(
    max_depth=8,
    learning_rate=0.05,
    max_iter=300,
    random_state=42
)

gb_model.fit(X_train_scaled, y_train)

validation_accuracy = gb_model.score(X_val_scaled, y_val)
print(f"\nGradient Boosting Validation Accuracy: {validation_accuracy:.4f}")
